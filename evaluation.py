# -*- coding: utf-8 -*-
"""eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QpuIKUuRNvUL-OqkstnMlvzPtUeKT_Z1
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
from sklearn.calibration import calibration_curve
import tensorflow as tf
from spektral.data import DisjointLoader
import model_and_train

def roc_auc(loader_te,model):
  y_true = []
  y_pred = []

  # Add the true values and the predicted values in the list
  for batch in loader_te:
      inputs, target = batch
      p = model(inputs, training=False)
      y_true.append(target)
      y_pred.append(p.numpy())

  # Calculate the ROC-AUC metric
  y_true = np.vstack(y_true)
  y_pred = np.vstack(y_pred)

  # Assuming y_true and y_pred are defined and populated as before

  fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)
  roc_auc = metrics.auc(fpr, tpr)

  sns.set(style="whitegrid")  # Use seaborn style for more polished aesthetics

  plt.figure(figsize=(8, 6), dpi=300)  # Opt for a sensible DPI
  plt.plot(fpr, tpr, color='magenta', lw=2, label='AUC = %0.2f' % roc_auc, marker='*', markersize=6)  # Vibrant color & markers
  plt.fill_between(fpr, tpr, color='lavender', alpha=0.4)  # Complementary fill color
  plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')  # Differentiate diagonal line

  # Enhance labels and title aesthetics
  plt.xlabel('False Positive Rate', fontsize=14, fontweight='bold')
  plt.ylabel('True Positive Rate', fontsize=14, fontweight='bold')
  plt.title('Receiver Operating Characteristic', fontsize=16, fontweight='bold')
  plt.legend(loc='lower right', fontsize=12)

  # Refine tick marks and grid for better readability
  plt.xticks(fontsize=12)
  plt.yticks(fontsize=12)
  plt.grid(True, which='major', linestyle='--', linewidth=0.5, color='gray')

  plt.tight_layout()
  plt.show()

  return roc_auc

def ece(data, matrix_type, model):
  # Reload the dataset in the CounterDataset format.
  dataset_c = model_and_train.CounterDataset(data=data, matrix_type = matrix_type)

  # Setup the loader.
  loader = DisjointLoader(dataset_c, batch_size=1, epochs=1, shuffle = False)

  # Set up an empty pandas dataframe.
  ece_df = pd.DataFrame(columns = ['output_pp', 'predicted', 'target', 'result'])

  # Compute the predictions and save them in the Pandas DataFrame.
  for batch in loader:
      inputs, target = batch
      p = model(inputs, training=False)
      original_prediction = p.numpy()[0][0]

      # Threshold set to 0.5
      predicted_value = 1 if original_prediction >= 0.5 else 0
      ece_df.loc[len(ece_df)] = [original_prediction, predicted_value, target[0][0], 1 if predicted_value == target[0][0] else 0]

  # Setting up the bins
  bin_ranges = [(0, 0.1), (0.1, 0.2), (0.2, 0.3), (0.3, 0.4), (0.4, 0.5), (0.5, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)]
  bin_calc = pd.DataFrame(columns = ['bin', 'count', 'accuracy', 'avg_pp', 'acc-conf', 'count_into_acc-conf'])

  for i, bin_range in enumerate(bin_ranges):
      # Get the higher and lower end of the bins
      lower, higher = bin_range[0], bin_range[1]

      # Get the probability outputs within the range
      bin_calc_temp = ece_df.loc[(ece_df['output_pp'] > lower) & (ece_df['output_pp'] <= higher)]
      count = bin_calc_temp.shape[0]

      # Compute parameters needed to calculate ECE
      if count > 0:
          total_corrects = bin_calc_temp[(bin_calc_temp['result'] == 1)].shape[0]
          accuracy = total_corrects / count
          avg_pp = bin_calc_temp['output_pp'].mean()
          acc_conf = abs(accuracy - avg_pp)

          bin_calc.loc[i] = [bin_range, count, accuracy, avg_pp, acc_conf, count*acc_conf]

  # Print ECE value
  print("ECE is : " + str(bin_calc['count_into_acc-conf'].sum() / bin_calc['count'].sum()))

  cal_y, cal_x = calibration_curve(ece_df['target'], ece_df['output_pp'], n_bins=10)

  sns.set_style("whitegrid")  # Use seaborn style for a cleaner background and grid

  fig, ax = plt.subplots(figsize=(10, 7))  # Slightly larger figure size for better detail visibility
  # Plot the calibration curve with aesthetic improvements
  plt.plot(cal_x, cal_y, marker='o', markersize=12, linestyle='-', linewidth=3, color='royalblue', label='Model Calibration')
  plt.plot([0, 1], [0, 1], ls='--', color='darkorange', linewidth=2, label='Ideal Calibration')
  plt.legend(loc='upper left', fontsize=12, frameon=True, facecolor='white', edgecolor='black')
  plt.xlabel('Average Predicted Probability in Each Bin', fontsize=14, fontweight='bold')
  plt.ylabel('Ratio of Positives', fontsize=14, fontweight='bold')
  plt.title("Calibration Curve", fontsize=16, fontweight='bold')
  plt.grid(True, which='both', linestyle='-', color='grey', linewidth=0.5, alpha=0.5)  # Enhanced grid visibility
  plt.grid(True, which='minor', linestyle=':', linewidth='0.5', color='grey', alpha=0.5)  # Add minor grid lines
  plt.minorticks_on()  # Enable minor tcicks
  plt.tight_layout()
  plt.show()

  return bin_calc['count_into_acc-conf'].sum() / bin_calc['count'].sum()