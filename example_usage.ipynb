{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install spektral\n",
        "! pip install livelossplot"
      ],
      "metadata": {
        "id": "Kp3uEP-TdF4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K9WwaeacO8p",
        "outputId": "bbe9f5b2-353a-4a4b-c9bc-d4bd7113a1f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/SMGT430/Understanding-Counterattack-in-Soccer-by-GCN')"
      ],
      "metadata": {
        "id": "qeE7bU8HcX81"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python /content/drive/MyDrive/SMGT430/Understanding-Counterattack-Soccer-by-GCN/run.py --model 'GNN' --dataset 'combined.pkl' --edge_feature 0 1 2 5 --node_feature 11 7 8 6 4 3 2 0 --matrix_type 'normal' --learning_rate 0.0001 --epochs 200 --batch_size 16 --channels 128 --layers 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi8TJnMLcpH2",
        "outputId": "b058baec-154a-4b64-ef3c-de70111e8ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-02 07:11:42.986988: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-02 07:11:42.987071: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-02 07:11:42.991039: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-02 07:11:43.014003: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-02 07:11:45.514365: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/spektral/data/utils.py:221: UserWarning: you are shuffling a 'CounterDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
            "  np.random.shuffle(a)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "Loss: 14.642338752746582\n",
            "Loss: 1.4156193733215332\n",
            "Loss: 0.9393652677536011\n",
            "Loss: 0.7839742302894592\n",
            "Loss: 0.7311188578605652\n",
            "Loss: 0.7019708752632141\n",
            "Loss: 0.6848231554031372\n",
            "Loss: 0.6696475148200989\n",
            "Loss: 0.6656394600868225\n",
            "Loss: 0.659142017364502\n",
            "Loss: 0.6572719216346741\n",
            "Loss: 0.6539331674575806\n",
            "Loss: 0.6511069536209106\n",
            "Loss: 0.6495059728622437\n",
            "Loss: 0.6453385949134827\n",
            "Loss: 0.644138753414154\n",
            "Loss: 0.6443169713020325\n",
            "Loss: 0.6421198844909668\n",
            "Loss: 0.6428111791610718\n",
            "Loss: 0.6409071087837219\n",
            "Loss: 0.6379439234733582\n",
            "Loss: 0.6395601034164429\n",
            "Loss: 0.6378480195999146\n",
            "Loss: 0.6375207304954529\n",
            "Loss: 0.638329803943634\n",
            "Loss: 0.635722815990448\n",
            "Loss: 0.6348466277122498\n",
            "Loss: 0.632740318775177\n",
            "Loss: 0.6334670782089233\n",
            "Loss: 0.633260190486908\n",
            "Loss: 0.6332640051841736\n",
            "Loss: 0.6319212913513184\n",
            "Loss: 0.6315560936927795\n",
            "Loss: 0.6307244300842285\n",
            "Loss: 0.6298195719718933\n",
            "Loss: 0.6291473507881165\n",
            "Loss: 0.6290971636772156\n",
            "Loss: 0.6280468106269836\n",
            "Loss: 0.6294327974319458\n",
            "Loss: 0.6286792755126953\n",
            "Loss: 0.6288836598396301\n",
            "Loss: 0.6277201771736145\n",
            "Loss: 0.625720739364624\n",
            "Loss: 0.6262997984886169\n",
            "Loss: 0.6264925599098206\n",
            "Loss: 0.6247110366821289\n",
            "Loss: 0.6256189346313477\n",
            "Loss: 0.6252771019935608\n",
            "Loss: 0.6242046356201172\n",
            "Loss: 0.6238805651664734\n",
            "Loss: 0.6253162622451782\n",
            "Loss: 0.6244467496871948\n",
            "Loss: 0.6230183839797974\n",
            "Loss: 0.6247872114181519\n",
            "Loss: 0.622707724571228\n",
            "Loss: 0.621711015701294\n",
            "Loss: 0.6205736398696899\n",
            "Loss: 0.6221838593482971\n",
            "Loss: 0.6221374273300171\n",
            "Loss: 0.6217154860496521\n",
            "Loss: 0.6209938526153564\n",
            "Loss: 0.6205340623855591\n",
            "Loss: 0.6204482316970825\n",
            "Loss: 0.6210790872573853\n",
            "Loss: 0.6190455555915833\n",
            "Loss: 0.6197097897529602\n",
            "Loss: 0.6209360957145691\n",
            "Loss: 0.6186434030532837\n",
            "Loss: 0.620066225528717\n",
            "Loss: 0.6187843680381775\n",
            "Loss: 0.617579996585846\n",
            "Loss: 0.618090808391571\n",
            "Loss: 0.6185668706893921\n",
            "Loss: 0.6198896169662476\n",
            "Loss: 0.6184444427490234\n",
            "Loss: 0.6177400350570679\n",
            "Loss: 0.6156746745109558\n",
            "Loss: 0.6171831488609314\n",
            "Loss: 0.6152879595756531\n",
            "Loss: 0.6175780296325684\n",
            "Loss: 0.6172921061515808\n",
            "Loss: 0.6144397854804993\n",
            "Loss: 0.616659939289093\n",
            "Loss: 0.6148543953895569\n",
            "Loss: 0.6153824329376221\n",
            "Loss: 0.6151044368743896\n",
            "Loss: 0.61449134349823\n",
            "Loss: 0.6142714619636536\n",
            "Loss: 0.6143573522567749\n",
            "Loss: 0.6120327115058899\n",
            "Loss: 0.6138575077056885\n",
            "Loss: 0.6130420565605164\n",
            "Loss: 0.6142120957374573\n",
            "Loss: 0.6136413216590881\n",
            "Loss: 0.6128002405166626\n",
            "Loss: 0.6119227409362793\n",
            "Loss: 0.6135830283164978\n",
            "Loss: 0.6116873621940613\n",
            "Loss: 0.6118447780609131\n",
            "Loss: 0.6102595329284668\n",
            "Loss: 0.6109756827354431\n",
            "Loss: 0.6101266741752625\n",
            "Loss: 0.6097180247306824\n",
            "Loss: 0.6086814403533936\n",
            "Loss: 0.6098475456237793\n",
            "Loss: 0.6085405945777893\n",
            "Loss: 0.611283004283905\n",
            "Loss: 0.6096975207328796\n",
            "Loss: 0.610093355178833\n",
            "Loss: 0.609786331653595\n",
            "Loss: 0.60997074842453\n",
            "Loss: 0.6091265678405762\n",
            "Loss: 0.6089305877685547\n",
            "Loss: 0.6084595918655396\n",
            "Loss: 0.6075496673583984\n",
            "Loss: 0.6071701645851135\n",
            "Loss: 0.6086663603782654\n",
            "Loss: 0.6089308857917786\n",
            "Loss: 0.6058088541030884\n",
            "Loss: 0.6079816818237305\n",
            "Loss: 0.607551634311676\n",
            "Loss: 0.6063150763511658\n",
            "Loss: 0.6059555411338806\n",
            "Loss: 0.6070187091827393\n",
            "Loss: 0.6056296229362488\n",
            "Loss: 0.605640172958374\n",
            "Loss: 0.605962336063385\n",
            "Loss: 0.6054850816726685\n",
            "Loss: 0.6069270968437195\n",
            "Loss: 0.6056377291679382\n",
            "Loss: 0.6050463318824768\n",
            "Loss: 0.6040517091751099\n",
            "Loss: 0.60521000623703\n",
            "Loss: 0.6023405194282532\n",
            "Loss: 0.60417640209198\n",
            "Loss: 0.6041086316108704\n",
            "Loss: 0.604960024356842\n",
            "Loss: 0.6048557162284851\n",
            "Loss: 0.6032095551490784\n",
            "Loss: 0.6032626628875732\n",
            "Loss: 0.6024072170257568\n",
            "Loss: 0.603012204170227\n",
            "Loss: 0.6031240224838257\n",
            "Loss: 0.60367751121521\n",
            "Loss: 0.6027605533599854\n",
            "Loss: 0.602564811706543\n",
            "Loss: 0.6029345989227295\n",
            "Loss: 0.6010748147964478\n",
            "Loss: 0.6019209623336792\n",
            "Loss: 0.5998657941818237\n",
            "Loss: 0.60088050365448\n",
            "Loss: 0.6007053256034851\n",
            "Loss: 0.6017577052116394\n",
            "Loss: 0.6003996133804321\n",
            "Loss: 0.6007063388824463\n",
            "Loss: 0.5999568104743958\n",
            "Loss: 0.6007995009422302\n",
            "Loss: 0.6000916957855225\n",
            "Loss: 0.5974699258804321\n",
            "Loss: 0.5970892906188965\n",
            "Loss: 0.5999775528907776\n",
            "Loss: 0.6000253558158875\n",
            "Loss: 0.5978622436523438\n",
            "Loss: 0.598537802696228\n",
            "Loss: 0.5985873937606812\n",
            "Loss: 0.5970315933227539\n",
            "Loss: 0.5981341004371643\n",
            "Loss: 0.5978227853775024\n",
            "Loss: 0.5969634652137756\n",
            "Loss: 0.5975449085235596\n",
            "Loss: 0.5960282683372498\n",
            "Loss: 0.5967411994934082\n",
            "Loss: 0.5981426239013672\n",
            "Loss: 0.5959378480911255\n",
            "Loss: 0.5965463519096375\n",
            "Loss: 0.5959908366203308\n",
            "Loss: 0.5966985821723938\n",
            "Loss: 0.5970122218132019\n",
            "Loss: 0.5972792506217957\n",
            "Loss: 0.5966936945915222\n",
            "Loss: 0.5980852246284485\n",
            "Loss: 0.5956226587295532\n",
            "Loss: 0.5942877531051636\n",
            "Loss: 0.5947746634483337\n",
            "Loss: 0.5965240001678467\n",
            "Loss: 0.5960941314697266\n",
            "Loss: 0.5964625477790833\n",
            "Loss: 0.5950940251350403\n",
            "Loss: 0.5970339179039001\n",
            "Loss: 0.5944716334342957\n",
            "Loss: 0.593548059463501\n",
            "Loss: 0.5928863286972046\n",
            "Loss: 0.5935863852500916\n",
            "Loss: 0.5920971632003784\n",
            "Loss: 0.5931457281112671\n",
            "Loss: 0.593959629535675\n",
            "Loss: 0.5938048362731934\n",
            "Loss: 0.5915904641151428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python /content/drive/MyDrive/SMGT430/Understanding-Counterattack-Soccer-by-GCN/run.py --model 'GCN' --dataset 'combined.pkl' --matrix_type 'normal' --learning_rate 0.0001 --epochs 200 --batch_size 16 --channels 128 --layers 3"
      ],
      "metadata": {
        "id": "k7oiCdKQPHSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d520de6f-a181-452d-9e9a-78e1f6750aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-02 08:28:40.028377: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-02 08:28:40.028481: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-02 08:28:40.031294: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-02 08:28:40.057580: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-02 08:28:42.531071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/spektral/data/utils.py:221: UserWarning: you are shuffling a 'CounterDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
            "  np.random.shuffle(a)\n",
            "Loss: 9.432487487792969\n",
            "Loss: 0.8248018622398376\n",
            "Loss: 0.7206500768661499\n",
            "Loss: 0.702987015247345\n",
            "Loss: 0.6936722993850708\n",
            "Loss: 0.6856839060783386\n",
            "Loss: 0.6793222427368164\n",
            "Loss: 0.6726359128952026\n",
            "Loss: 0.671613872051239\n",
            "Loss: 0.6637228727340698\n",
            "Loss: 0.664626955986023\n",
            "Loss: 0.6606278419494629\n",
            "Loss: 0.6608450412750244\n",
            "Loss: 0.6541569232940674\n",
            "Loss: 0.6507952213287354\n",
            "Loss: 0.6487398147583008\n",
            "Loss: 0.6485753059387207\n",
            "Loss: 0.6450546383857727\n",
            "Loss: 0.643682062625885\n",
            "Loss: 0.642485499382019\n",
            "Loss: 0.6424529552459717\n",
            "Loss: 0.6397034525871277\n",
            "Loss: 0.6383731365203857\n",
            "Loss: 0.6367238163948059\n",
            "Loss: 0.637570321559906\n",
            "Loss: 0.635982871055603\n",
            "Loss: 0.6347059011459351\n",
            "Loss: 0.6354749202728271\n",
            "Loss: 0.6341643929481506\n",
            "Loss: 0.6323020458221436\n",
            "Loss: 0.6311098337173462\n",
            "Loss: 0.6318501830101013\n",
            "Loss: 0.6302787065505981\n",
            "Loss: 0.6311336159706116\n",
            "Loss: 0.628573477268219\n",
            "Loss: 0.6273589134216309\n",
            "Loss: 0.6276441812515259\n",
            "Loss: 0.62788987159729\n",
            "Loss: 0.625968873500824\n",
            "Loss: 0.624416172504425\n",
            "Loss: 0.6267315745353699\n",
            "Loss: 0.6244911551475525\n",
            "Loss: 0.6257758140563965\n",
            "Loss: 0.6225146651268005\n",
            "Loss: 0.6224529147148132\n",
            "Loss: 0.6201336979866028\n",
            "Loss: 0.621846616268158\n",
            "Loss: 0.6199519634246826\n",
            "Loss: 0.6192180514335632\n",
            "Loss: 0.6179438233375549\n",
            "Loss: 0.6171472072601318\n",
            "Loss: 0.6170449256896973\n",
            "Loss: 0.6169527173042297\n",
            "Loss: 0.6150620579719543\n",
            "Loss: 0.6143532991409302\n",
            "Loss: 0.614406406879425\n",
            "Loss: 0.6135783195495605\n",
            "Loss: 0.6123331189155579\n",
            "Loss: 0.6114543676376343\n",
            "Loss: 0.6109473705291748\n",
            "Loss: 0.6108542680740356\n",
            "Loss: 0.6076669692993164\n",
            "Loss: 0.6099526882171631\n",
            "Loss: 0.6081942915916443\n",
            "Loss: 0.6070705056190491\n",
            "Loss: 0.6061467528343201\n",
            "Loss: 0.6061533093452454\n",
            "Loss: 0.6047529578208923\n",
            "Loss: 0.60579913854599\n",
            "Loss: 0.60256427526474\n",
            "Loss: 0.6016635894775391\n",
            "Loss: 0.5983853936195374\n",
            "Loss: 0.5995067358016968\n",
            "Loss: 0.598056435585022\n",
            "Loss: 0.5959143042564392\n",
            "Loss: 0.5941855907440186\n",
            "Loss: 0.5939180850982666\n",
            "Loss: 0.5960890650749207\n",
            "Loss: 0.5929518938064575\n",
            "Loss: 0.5904860496520996\n",
            "Loss: 0.5894008874893188\n",
            "Loss: 0.5886999368667603\n",
            "Loss: 0.5885103940963745\n",
            "Loss: 0.5883674025535583\n",
            "Loss: 0.5846894383430481\n",
            "Loss: 0.5857160091400146\n",
            "Loss: 0.5845905542373657\n",
            "Loss: 0.584174633026123\n",
            "Loss: 0.5819466710090637\n",
            "Loss: 0.5773686766624451\n",
            "Loss: 0.5759655833244324\n",
            "Loss: 0.5783975124359131\n",
            "Loss: 0.5744171142578125\n",
            "Loss: 0.5743446946144104\n",
            "Loss: 0.5721457004547119\n",
            "Loss: 0.5701541900634766\n",
            "Loss: 0.5692088007926941\n",
            "Loss: 0.5674493312835693\n",
            "Loss: 0.5663062334060669\n",
            "Loss: 0.565385639667511\n",
            "Loss: 0.5638219118118286\n",
            "Loss: 0.5618367195129395\n",
            "Loss: 0.5643149614334106\n",
            "Loss: 0.5597020983695984\n",
            "Loss: 0.5572274923324585\n",
            "Loss: 0.5567904710769653\n",
            "Loss: 0.5555772185325623\n",
            "Loss: 0.555366039276123\n",
            "Loss: 0.5530809164047241\n",
            "Loss: 0.5500508546829224\n",
            "Loss: 0.5498625040054321\n",
            "Loss: 0.5485503673553467\n",
            "Loss: 0.5453912019729614\n",
            "Loss: 0.5451623797416687\n",
            "Loss: 0.5461105704307556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqmnCNtGU4sk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}